---
layout: post
title: "TRPO算法详解"
description: "详细解释置信域策略优化算法（Trust Region Policy Optimization,TRPO）"
author:     "Hao"
category: true
tagline: 
tags: [algorithm]
---

[toc]

# 一、算法背景

大部分强化学习算法**很难保证单调收敛**，这使得即使参数空间中看似很小的差异也会在性能上产生非常大的差异结果，因此一个错误的步骤可能会导致策略性能崩溃。而TRPO通过采取**尽可能大的步骤提高性能来更新策略**，利用KL散度对新旧策略接近程度进行约束，避免了这种情况。





置信域策略优化算法（Trust Region Policy Optimization,TRPO）是一种基于策略的方法，即先对策略进行参数化，并设计衡量策略质量的指标或目标函数，然后通过梯度上升法来最大化这指标，让策略逼近局部最优。一般的策略梯度算法在沿着策略梯度更新参数时，可能因为步长太大，使策略变差。TRPO在更新参数的时候会先试探权重参数下一步要更新的位置是否失控，如果失控则调整步长，否则视该区域为置信域（Trust Region）,在该区域内能保障策略提升的单调性。

# 二、定义

## 策略评估(policy evaluation)

策略$\pi$下产生的一系列状态-动作对的预期累计回报：
$$
\eta(\pi)=E_{s_0,a_0,s_1,a_1,\cdots}[\sum_{t=0}^{\infty}\gamma^tr(s_t)]\\ 
其中，s_0为环境的初始状态，与策略无关，由环境自动生成，即s_0 \thicksim \rho(s_0)；\\
a_t\thicksim\pi(\cdot\mid s_t);s_{t+1} \thicksim P(s_{t+1}\mid s_t,a_t);\tag{1}
$$


## 状态值函数(state value function)

$$
V_{\pi}(s_t)=E_{a_t,s_{t+1},\cdots}\lbrack \sum_{l=0}^{\infty}\gamma^lr(s_{t+l}) \rbrack \tag{2}
$$

## 状态-动作值函数(state-action value function)

$$
Q_{\pi}(s_t,a_t)=E_{s_{t+1},a_{t+1},\cdots}[\sum_{l=0}^{\infty}\gamma^lr(s_{t+l})] \tag{3}
$$

## 动作优势函数(advantage action function)

即状态s下使用动作a产生的回报与状态s时所有动作产生平均回报的差，衡量某个特定动作相对平均收益的优势
$$
A_{\pi}(s,a)=Q_{\pi}(s,a)-V_{\pi}(s) \tag{4}
$$

# 三、将新策略的回报表示为旧策略的回报+其他值

$$
& \eta(\widetilde{\pi})=\eta(\pi)+E_{s_0,a_0,\cdots \thicksim\widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^tA_{\pi}(s_t,a_t)]\\
& 其中,s_0 \thicksim \rho(s_0),a_t\thicksim\pi(\cdot\mid s_t),s_{t+1} \thicksim P(s_{t+1}\mid s_t,a_t) \tag{5}
$$

> 证明：
> $$
> \begin{aligned}
> & E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}A_{\pi}(s_t,a_t)] \\
> &=E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}[Q_{\pi}(s_t,a_t)-V_{\pi}(s_t)]]\\
> &=E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}(r(s_t)+\gamma V_\pi(s_{t+1})-V_\pi (s_t))]\\
> &=E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}r(s_t)]+E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}(\gamma V_\pi(s_{t+1})-V_\pi (s_t))]\\
> &=\eta(\widetilde{\pi})+E_{\tau \mid \widetilde{\pi}}[-V_{\widetilde{\pi}}(s_0)+\gamma V_{\widetilde{\pi}}(s_1)-\gamma V_{\widetilde{\pi}}(s_1)+\gamma^2 V_{\widetilde{\pi}}(s_2)+\cdots]\\
> &=\eta(\widetilde{\pi})+(-E_{s_0}[V_{\pi}(s_0)])\longrightarrow 此处s_0\thicksim \pi 等价于 s_0 \thicksim \widetilde{\pi}\\
> &=\eta(\widetilde{\pi})-\eta(\pi)\\
> 其中：\\
> & E_{s_0}[V_{\pi}(s_0)]\\
> &=E_{s_0}[E_{a_0,s_1,\cdots}[\sum_{t=0}^{\infty}\gamma^t r(s_{0+t})]]\\
> &=E_{a_0,s_1,\cdots}[\sum_{t=0}^{\infty}\gamma^t r(s_{0+t})]\\
> &=\eta(\pi)\\
> 证毕
> \end{aligned}
> $$
>

定义：
$$
\rho_{\pi}(s)=P(s_0=s)+\gamma P(s_1=s)+\gamma^2 P(s_2=s)+\cdots \tag{6}
$$
即每个状态的（未标准化的）折扣访问频率（Discounted Visitation Frequencies），其将时间步上的累加，转为了状态上的累加。当$\gamma$为1时，可以将其理解为状态的占用度量。

将该定义带入式（5），得到：
$$
\eta(\widetilde{\pi})=\eta(\pi)+\sum_{s}\rho_{\widetilde{\pi}}(s)\sum_{a}\widetilde{\pi}(a|s)A_{\pi}(s,a) \tag{7}
$$

> 证明：
> $$
> \begin{aligned}
> \eta(\widetilde{\pi})&=\eta(\pi)+\sum_{t=0}^{\infty}\sum_{s}P(s_t=s|\widetilde{\pi})\sum_{a}[\widetilde{\pi}(a|s) \cdot \gamma ^t \cdot A_{\pi}(s,a)]\\
> &=\eta(\pi)+\sum_{s}\sum_{t=0}^{\infty}\gamma ^tP(s_t=s|\widetilde{\pi})\sum_{a}[\widetilde{\pi}(a|s) \cdot A_{\pi}(s,a)]\\
> &=\eta(\pi)+\sum_{s}\rho_{\widetilde{\pi}}(s) \cdot \sum_a\widetilde{\pi}(a|s) \cdot A_{\pi}(s,a)
> \end{aligned}
> $$

# 四、对$\eta(\widetilde{\pi})$近似，获得替代回报函数

如果先大量采样得到$\rho_{\widetilde{\pi}}(s)$，再验证式（7）右边第二项$\ge$0，计算量太大，需要对不同的$\widetilde{\pi}$都进行大量采样,也就是盲目地选择一个策略，然后大量采样，看看式（7）第二项是否大于0，这种方法显然是不现实的，而强化学习的目标就是减少采样次数。

考虑这样一种情况，将原回报函数中的$\rho_{\widetilde{\pi}}(s)$替换为$\rho_{\pi}(s)$，定义替换函数：
$$
L_{\pi}(\widetilde{\pi})=\eta(\pi)+\sum_{s}\rho_{\pi}(s)\cdot \sum_a\widetilde{\pi}(a|s)  A_{\pi}(s,a) \tag{8}
$$
当$\eta(\widetilde{\pi})$和$L_{\pi}(\widetilde{\pi})$相差很小时，两者可以相互替代，将其均看成$\widetilde{\pi}$的函数，$\widetilde{\pi}$和$\pi$均为$\theta$的函数，当$\eta(\widetilde{\pi})$和$L_{\pi}(\widetilde{\pi})$在$\pi_{\theta_{old}}$处一阶近似时，即：
$$
\begin{array}{l}
L_{\pi_{\theta \text { old }}}\left(\pi_{\theta_{\text {old }}}\right)=\eta\left(\pi_{\theta_{\text {old }}}\right) \tag{9} \\
\left.\nabla_{\theta} L_{\pi_{\theta_{\text {old }}}}\left(\pi_{\theta}\right)\right|_{\theta=\theta_{\text {old }}}=\left.\nabla_{\theta} \eta\left(\pi_{\theta}\right)\right|_{\theta=\theta_{\text {old }}}
\end{array}
$$

> 证明：
>
> 1）对于式(9)的第一个式子：
>
> ​			式（7）中的$\pi$和$\pi_{old}$是一样的，都是指的原来的策略，故：
> $$
> \eta(\pi_{old})=\eta(\pi_{old})+\sum_{s}\rho_{\pi_{old}}(s) \sum_a \pi_{old}(a|s) A_{\pi_{old}}(s,a)
> $$
> ​			其中,
> $$
> \sum_a \pi_{old}(a|s)A_{\pi_{old}}(s,a)=0
> $$
> ​			上式等号右边第二项为0，故式（9）第一个式子得证。
>
> 2）对于式（9）的第二个式子，分别让式（7）和（8）对$\theta$求偏导，得：

> $$
> \begin{aligned}
> \nabla_{\theta}\eta(\widetilde{\pi})|_{\theta=\theta_{\text {old }}}&=\nabla_{\theta} \eta(\pi_{\pi_{\theta_{old}}})+\sum_s \nabla_{\theta}\rho_{\widetilde{\pi}}(s)\sum_{a}\widetilde{\pi}(a|s)A_{\pi}(s,a)+\sum_{s}\rho_{\widetilde{\pi}}(s)\sum_{a}\nabla\widetilde{\pi}(a|s)A_{\pi}(s,a)  \\
> &(1)第一项\eta(\pi_{\theta_{old}})是常数，故\nabla\eta(\pi_{\theta_{old}})=0\\
> &(2)当\widetilde{\pi}=\pi_{old}时，即\widetilde{\pi}的参数\theta等于\pi_{old}的参数\theta_{old}时，\sum_{a}\widetilde{\pi}(a|s)A_{\pi}(s,a)=0，故有\\
> &=\sum_{s}\rho_{\widetilde{\pi}}(s)\sum_{a}\nabla\widetilde{\pi}(a|s)A_{\pi}(s,a)\\
> &代入\theta=\theta_{old}，即\widetilde{\pi}=\pi_{old}\\
> &=\sum_{s}\rho_{\pi_{\theta_{old}}}(s)\sum_{a}\nabla \widetilde{\pi}(a|s)A_{\pi_{\theta_{old}}}(s,a)\\
> \end{aligned}
> $$
>
> $$
> \nabla_{\theta}L_{\pi_{\theta_{old}}}(\widetilde{\pi})|_{\theta=\theta_{\text {old }}}=0+\sum_{s}\rho_{\pi_{\theta_{old}}}(s)\sum_{a}\nabla \widetilde{\pi}(a|s)A_{\pi_{\theta_{old}}}(s,a) \longrightarrow 原式第一项\eta(\pi_{\theta_{old}})是常数，故\nabla\eta(\pi_{\theta_{old}})=0
> $$
>
> 证毕。

当$\eta(\widetilde{\pi})$和$L_{\pi}(\widetilde{\pi})$在$\pi_{\theta_{old}}$处一阶近似时，则在$\pi_{old}$附近，改善L的策略也能改善原汇报函数$\eta$，只要步长控制在$\pi_{old}$合理的邻域内。

# 五、控制$\pi$和$\widetilde{\pi}$之间的散度小于$\alpha$，就能保证回报单调增长

如果要使用L回报函数替代$\eta$回报函数，则$\pi$和$\widetilde{\pi}$不能差太多，否则一阶近似邻域将非常小，导致极其小的步长，会使得训练变慢。

Kakade和Langford在2002年提出过一个保守策略迭代更新方案，可以为$\eta$更新提供明确的下界，即对于策略改进采用以下混合方式时：
$$
\pi_{new}(a|s)=(1-\alpha)\pi_{old}(a|s)+\alpha \pi'(a|s) \tag{10}
$$
其中，$\pi'=arg \min_{\pi'}L_{\pi_{old}}(\pi')$，有
$$
\eta(\pi_{new})\ge L_{\pi_{old}}-\frac{2\epsilon \gamma}{(1-\gamma)^2}\alpha^2,\epsilon=\max_{s}|E_{a \thicksim \widetilde{\pi}(a|s) }[A_{\pi}(s,a)]| \tag{11}
$$

> 证明：
>
> 首先定义$\overline{A}(s)$：
> $$
> \overline{A}(s)=E_{a \thicksim \widetilde{\pi}(a|s) }[A_{\pi}(s,a)]
> $$
> $\overline{A}(s)$表示在状态s时采用策略$\widetilde{\pi}$相对于之前策略的改进。
>
> 用$\overline{A}(s)$改写式(7)和(8)，得到：
> $$
> \begin{array}{l}
> \eta(\tilde{\pi})=\eta(\pi)+E_{\tau \sim \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} \bar{A}\left(s_{t}\right)\right] \\
> L_{\pi}(\tilde{\pi})=\eta(\pi)+E_{\tau \sim \pi}\left[\sum_{t=0}^{\infty} \gamma^{t} \bar{A}\left(s_{t}\right)\right]
> \end{array}
> $$
> 由于策略按照 $\pi_{new}(a|s)=(1-\alpha)\pi_{old}(a|s)+\alpha \pi^`(a|s)的模式混合，假设新策略\widetilde{\pi}是由\pi_{old}和\pi^`$各自按照一定权重进行混合的，策略可以表示为策略对$(\pi,\widetilde{\pi})$，由策略对产生的动作对$(a,\widetilde{a})$。
>
> 从这种视角看，产生动作$\widetilde{a}$的概率为$\alpha$,因为不同策略也可能产生相同的动作，所以在改进的策略$\pi_{new}$中，产生和原策略的动作（即a）不同的概率最多为$\alpha$，即$P(a\neq \widetilde{a}|s)\le\alpha$。
>
> 于是有：
> $$
> \begin{aligned}
> \overline{A}&=E_{\widetilde{a} \sim \widetilde{\pi}}[A_{\pi}(s,\widetilde{a})] \longrightarrow 定义\\
> &=E_{(a,\widetilde{a}) \sim (\pi,\widetilde{\pi})}[A_{\pi}(s,\widetilde{a})-A_{\pi}(s,a)] \longrightarrow E_{a \sim \pi}A_{\pi}(s,a)=0，所以这个等号就是减去了0\\
> &=P(a \ne \widetilde{a}|s)E_{(a,\widetilde{a}) \sim (\pi,\widetilde{\pi})|a \ne \widetilde{a}}[A_{\pi}(s,\widetilde{a})-A_{\pi}(s,a)]\longrightarrow 当a=\widetilde{a}时,A_{\pi}(s,\widetilde{a})-A_{\pi}(s,a)=0\\
> \end{aligned}
> $$
> 于是有：
> $$
> |\overline{A}|\le P(a \ne \widetilde{a}|s)(|E_{\widetilde{a} \sim \widetilde{\pi}}[A_{\pi}(s,\widetilde{a})|+|E_{a \sim \pi}A_{\pi}(s,a)]|)\le \alpha \cdot 2 \cdot \max_{s,a}|A_{\pi}(s,a)| \tag{12}
> $$
> 用$n_t$表示在时刻t之前策略$\pi$和$\widetilde{\pi}$产生的不同动作的数量，有：
> $$
> E_{s_t\sim\widetilde{\pi}}\left[\overline{A}\left(s_i\right)\right]=P\left(n_t=0\right)E_{s_t\sim\widetilde{\pi}| n_t=0}\left[\overline{A}\left(s_t\right)\right]+P\left(n_t>0\right)E_{s_t\sim\widetilde{\pi}| n_t>0}\left[\overline{A}\left(s_t\right)\right] \tag{13}
> $$
> 和
> $$
> E_{s_t\sim \pi}\left[\overline{A}\left(s_i\right)\right]=P\left(n_t=0\right)E_{s_t\sim \pi | n_t=0}\left[\overline{A}\left(s_t\right)\right]+P\left(n_t>0\right)E_{s_t\sim \pi | n_t>0}\left[\overline{A}\left(s_t\right)\right] \tag{14}
> $$
> $n_t=0$时，策略$\pi$和$\widetilde{\pi}$动作相同，将到达相同的状态，则有：
> $$
> E_{s_t\sim\widetilde{\pi}| n_t=0}\left[\overline{A}\left(s_t\right)\right]=E_{s_t\sim \pi | n_t=0}\left[\overline{A}\left(s_t\right)\right]
> $$
> 则式（13）减去式（14）有：
> $$
> \begin{aligned}
> |E_{s_t\sim\widetilde{\pi}| n_t>0}\left[\overline{A}\left(s_t\right)\right]-E_{s_t\sim \pi | n_t>0}\left[\overline{A}\left(s_t\right)\right]| &\le  |E_{s_t\sim\widetilde{\pi}| n_t>0}\left[\overline{A}\left(s_t\right)\right]|+|E_{s_t\sim \pi | n_t>0}\left[\overline{A}\left(s_t\right)\right]| \longrightarrow 使用式(12)的结论\\
> &\le 4\alpha \max_{s,a}|A_{\pi}(s,a)|
> \end{aligned}
> $$
> 又$P(n_t =0) \ge(1-\alpha)^{t}$，故$P(n_t>0) \le 1-(1-\alpha)^t$
>
> 于是有：
> $$
> \begin{aligned}
> |E_{s_t\sim\widetilde{\pi}}\left[\overline{A}\left(s_i\right)\right]-E_{s_t\sim \pi}\left[\overline{A}\left(s_i\right)\right]|&=P(n_t>0)|E_{s_t\sim\widetilde{\pi}| n_t>0}\left[\overline{A}\left(s_t\right)\right]-E_{s_t\sim \pi | n_t>0}\left[\overline{A}\left(s_t\right)\right]|\\
> & \le(1-(1-\alpha)^t) \cdot 4\alpha \max_{s,a}|A_{\pi}(s,a)|
> \end{aligned}
> $$
> 从而有：
> $$
> \begin{aligned}
> |\eta(\widetilde{\pi})-L_{\pi}(\widetilde{\pi})|&=\sum_{t=0}^{\infty}\gamma^t|E_{\tau \sim \widetilde{\pi}}[\overline{A}(s_t)]-E_{\tau \sim \pi}[\overline{A}(s_t)]|\\
> &\le \sum_{t=0}^{\infty}\gamma^t \cdot 4\epsilon\alpha(1-(1-\alpha)^t) \longrightarrow \epsilon=\max_s |A_{\pi}(s,a)| 而不是\max_{s}|E_{a \thicksim \widetilde{\pi}(a|s) }[A_{\pi}(s,a)]|\\
> &=4\epsilon\alpha(\frac{1}{1-\gamma}-\frac{1}{1-\gamma(1-\alpha)})\\
> &=\frac{4\alpha^2\gamma\epsilon}{(1-\gamma)(1-\gamma(1-\alpha))}\\
> &\le\frac{4\alpha^2\gamma\epsilon}{(1-\gamma)^2} 
> \end{aligned}
> $$
> 证毕。

由式（11）可知，可以保证在一定的误差范围内可以用$L_{\pi}(\widetilde{\pi})$代替$\eta(\widetilde{\pi})$。

但混合策略，即式（12）在实际应用中用得很少，个人理解是超参数$\alpha$很难设定，而$\alpha$其实表征的是两个策略之间的距离，而策略其实就是概率分布，衡量两个概率分布的相似程度自然而然想到散度，于是使用总方差散度(the Total Variation divergence)。

对于离散的取值，我们有：
$$
D_{TV}(p||q)=\frac{1}{2}\sum_{i}|p_i-q_i|\\
D_{TV}^{max}(\pi,\widetilde{\pi})=\max_s(\pi(\cdot|s)||\widetilde{\pi}(\cdot|s)) \tag{15}
$$
又有：
$$
[D_{TV}(p||q)]^2 \le D_{KL}(p||q)\\
D_{KL}^{max}(\pi,\widetilde{\pi})=\max_{s} D_{KL}(\pi(\cdot|s)||\widetilde{\pi}(\cdot|s))
$$
从而有：
$$
\eta(\widetilde{\pi}) \ge L_{\pi}(\widetilde{\pi})-CD_{KL}^{max}(\pi,\widetilde{\pi})\\
\text{where C }=\frac{4\epsilon\gamma}{(1-\gamma)^2}
$$
可以用KL散度控制$\pi$和$\widetilde{\pi}$之间的距离小于$\alpha$时，就能够在误差确定的情况下使用$L_{\pi}(\widetilde{\pi})$替代$\eta(\widetilde{\pi})$，从而在优化$L_{\pi}(\widetilde{\pi})$时，$\eta(\widetilde{\pi})$也在优化。

在保证回报函数单调不减的情况下，求取更新策略的算法：

> 算法：保证预期回报$\eta$不减的近似策略迭代算法
>
> 输入：初始化策略$\pi_0$
>
> For i=0,1,2,3,$\cdots$ until 收敛 do:
>
> ​		计算优势函数$A_{\pi_i}(s,a)$
>
> ​		求解如下约束问题：
>
> ​				$\pi_{i+1}=arg max_{\pi}(L_{\pi_{i}}(\pi)-\frac{4\epsilon\gamma}{(1-\gamma)^2}D_{KL}^{max}(\pi_i,\pi))$,
>
> ​						其中$\epsilon=\max_{s}|E_{a \thicksim \widetilde{\pi}(a|s) }[A_{\pi}(s,a)]$
>
> ​								$L_{\pi_{i}}(\pi_i)=\eta(\pi_i)+\sum_{s}\rho_{\pi_i}(s)\sum_{a}\pi(a|s)A_{\pi_i}(s,a)$
>
> End for

证明以上算法的有效性：

> 设$M_i(\pi)=L_{\pi_i}(\pi)-CD_{KL}^{max}(\pi_i,\pi)$
>
> 则$M_i(\pi_i)=L_{\pi_i}(\pi_i)=\eta(\pi_i) \longrightarrow D_{KL}^{max}(\pi_i,\pi_i)=0$
>
> 取$\pi_{i+1}=arg max_{\pi}(L_{\pi_i}(\pi)-C \cdot D_{KL}^{max}(\pi_i,\pi))$
>
> 则$\eta(\pi_{i+1}) \ge M_{i+1}(\pi_{i+1})$
>
> 于是有 $\eta(\pi_{i+1})-\eta(\pi_i) \ge M_i(\pi_{i+1}-M_i(\pi_i))$
>
> 故改善$M_i$也会改善$\eta$
>
> 证毕。

# 六、采取重要性采样，Q函数替代A函数，对算法进一步近似

参数化的策略是通过改变参数来优化目标函数，即实现$\max_{\theta}(L_{\theta_{old}}(\theta)-C \cdot D_{KL}^{max}(\theta_{old},\theta))$，可以改写为：
$$
\max_\theta L_{\theta_{old}}(\theta)\\
\text{subject to} D_{KL}^{max}(\theta_{old},\theta) \le \delta \tag{16}
$$
但上式的约束太严格，要求状态空间的每一点都维持在KL散度在一定范围内，所以在实际应用中用平均散度来作为最大KL散度的近似，这样就可以使用采样的方法，即：
$$
\overline{D}_{KL}^{\rho}(\theta_1,\theta_2):=E_{s \sim \rho}[D_{KL}(\pi_{\theta_1(\cdot |s)}||\pi_{\theta_2(\cdot |s)})] \tag{17}
$$
则有：
$$
\max_\theta \sum_s \rho_{\theta_{old}}(s)\sum_a\pi_\theta(a|s)A_{\theta_{old}}(s,a)\\
\text{subject to } \overline{D}_{KL}^{\rho_{\theta_{old}}}(\theta_{old},\theta) \le \delta \tag{18}
$$
式（18）中的$\sum_s\rho_{\theta_{old}}(s)[\cdots]$可以根据其定义，使用$\frac{1}{1-\gamma}E_{s \sim \rho_{\theta_{old}}}[\cdots]$代替（将$\rho$定义中的$\gamma$考虑为权重，要让权重为1，则必须先乘上$1-\gamma$，然后再除以$1-\gamma$，这样$\sum_s\rho(s)=1$。

> 解释：
> $$
> \begin{aligned}
> \sum_s\rho_{\theta_{old}}(s)[\cdots]&=\sum_s\sum_t\gamma^tP(s_t|\pi_{\theta_{old}})[\cdots]\\
> &=\sum_t\gamma^t\sum_sP(s_t|\pi_{\theta_{old}})[\cdots]\\
> &\approx \frac{1}{1-\gamma} E_{s \sim \rho_{\theta_{old}}}[\cdots] \\
> \end{aligned}
> $$
> 

又因为式（18）第二个$\sum$的策略是按照新的策略，所以得引入重要性采样，用原策略采样得到的轨迹来训练

即：
$$
\sum_s\pi_{\theta}(a|s)A_{\theta_{old}}(s,a)=E_{a \sim \pi_{\theta_{old}}}[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}A_{\theta_{old}}(s,a)] \tag{19}
$$
再一个优化是用状态-动作价值函数Q(s,a)代替优势函数A(s,a)

> 解释：
> $$
> \begin{aligned}
> \sum_a\pi_\theta(a|s)A_{\theta_{old}}(s,a)&=\sum_a\pi_\theta(a|s)[Q_{\theta_{old}}(s)-V_{\theta_{old}}(s,a)]\\
> &=\sum_a[\pi_\theta(a|s)Q_{\theta_{old}}(s,a)]-V_{\theta_{old}}(s)\sum_a\pi_\theta(a|s)\\
> &=\sum_a[\pi_\theta(a|s)Q_{\theta_{old}}(s,a)]-V_{\theta_{old}}(s) \longrightarrow V_{\theta_{old}}(s)是常数
> \end{aligned}
> $$

原论中提到可以用Q替代A，但在代码实现中还是用A来实现的居多，应该是运用了类似Dueling DQN差不多的技巧，以加快训练速度。

最终TRPO的目标转化为转化为：
$$
\max_s E_{s \sim \rho_{\theta_{old}},a \sim \pi_{\theta_{old}}}[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}Q_{\theta_{old}}(s,a)]\\
\text{subject to }E_{s \sim \rho_{\theta_{old}}}[D_{KL}(\pi_{\theta_{old}}(\cdot|s)||\pi_{\theta}(\cdot|s))] \le \delta
\tag{20}
$$

# 七、对目标函数进行一阶逼近，对约束函数进行二阶逼近

纯理论上的TRPO更新不是最容易使用的，所以实际的TRPO算法进行了一些近似操作以快速获得答案。

1）对目标函数进行一阶逼近

记$L_{\theta_{old}}(\theta)=E_{s \sim \rho_{\theta_{old}},a \sim \pi_{\theta_{old}}}[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}A_{\theta_{old}}(s,a)]$

得到：
$$
\min_\theta -\nabla_\theta L_{\theta_{old}}(\theta)|_{\theta=\theta_{old}} \cdot (\theta-\theta_{old}) \tag{21}
$$

> 解释：
>
> 函数f(x)在x=a处的一阶泰勒展开为 $f(x)=f(a)+f^`(a)(x-a)$
>
> 故对式（20）的第一个式子在$\theta=\theta_{old}$处进行一阶泰勒展开，得到
> $$
> L_{\theta_{old}}(\theta)=L_{\theta_{old}}(\theta_{old})+\nabla_\theta L_{\theta_{old}}(\theta)|_{\theta=\theta_{old}} \cdot (\theta-\theta_{old})
> $$
> 显然等号第一项为0，最大化一个数等价于最小化它的相反数，且在机器学习中一般习惯于最小化目标函数
>
> 故
> $$
> \max_s E_{s \sim \rho_{\theta_{old}},a \sim \pi_{\theta_{old}}}[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}A_{\theta_{old}}(s,a)]\Leftrightarrow \min_\theta -(\nabla_\theta L_{\theta_{old}}(\theta)|_{\theta=\theta_{old}} \cdot (\theta-\theta_{old}))
> $$

2）对约束函数进行二阶逼近

得到：
$$
\frac{1}{2}(\theta-\theta_{old})^TF(\theta_{old})(\theta-\theta_{old}) \le \delta\\
其中F是费舍尔信息矩阵 \tag{22}
$$

> 解释：
>
> 根据KL散度的定义得：
> $$
> \begin{aligned}
> D_{KL}(\pi_{\theta_{old}}(\cdot|s)||\pi_{\theta}(\cdot|s))&=\int \pi_{\theta_{old}}(\cdot|s)\log\frac{\pi_{\theta_{old}}(\cdot|s)}{\pi_{\theta}(\cdot|s)}\,dx\\
> &=\int \pi_{\theta_{old}}(\cdot|s))\log\pi_{\theta_{old}}(\cdot|s))\,dx-\int \pi_{\theta_{old}}(\cdot|s))\log\pi_{\theta}(\cdot|s))\\
> &=E_{x \sim \pi_{\theta_{old}}}\log \pi_{\theta_{old}}-E_{x \sim \pi_{\theta_{old}}}\log \pi_{\theta}
> \end{aligned}
> $$
> 对$D_{KL}$进行一阶求导，即：
> $$
> \begin{aligned}
> \nabla_\theta D_{KL}(\pi_{\theta_{old}}(\cdot|s)||\pi_{\theta}(\cdot|s))&=-\int \pi_{\theta_{old}}(x|s))\nabla_{\theta}\log\pi_{\theta}(x|s))\,dx\\
> &=-\int \pi_{\theta_{old}}(x|s) \cdot\frac{\nabla_{\theta}\pi_{\theta}(x|s)}{\pi_{\theta(x|s)}}\,dx \longrightarrow代入\theta=\theta_{old}\\
> &=-\int\nabla_{\theta}\pi_{\theta_{old}}(x|s)\,dx\\
> &=-\nabla \int \pi_{\theta_{old}}(x|s)\,dx\\
> &=\nabla 常数\\
> &=0
> \end{aligned}
> $$
> 对$D_{KL}$进行二阶求导
> $$
> \begin{aligned}
> \nabla_\theta^2 D_{KL}(\pi_{\theta_{old}}(\cdot|s)||\pi_{\theta}(\cdot|s))|_{\theta=\theta_{old}}&=-\int \pi_{\theta_{old}}(x|s))\nabla_{\theta}^2\log\pi_{\theta}(x|s))\,dx|_{\theta=\theta_{old}} \longrightarrow 记H=\nabla_{\theta}^2\log\pi_{\theta}(x|s))|_{\theta=\theta_{old}}\\
> &=-\int \pi_{\theta_{old}}(x|s)H_{\log \pi_{\theta}}\,dx|_{\theta=\theta_{old}}\\
> &=-E_{\pi_{\theta_{old}}}[H_{\log \pi_{\theta_{old}}}]\\
> &=F \longrightarrow 费舍尔信息矩阵
> \end{aligned}
> $$
> 注：
>
> **两个重要结论**
> **结论1**：Fisher矩阵F是Hessian矩阵H的负期望
> $$
> F=-E_{p(x|\theta)}[\nabla_{\theta}\log p(x|\theta)\nabla \log p(x|\theta)^T]=-E_{p(x|\theta)}[H_{\log p(x|\theta)}]
> $$
> 当黑塞矩阵中的被微分的函数是对数函数时，其与费舍尔信息矩阵就相差一个负号
>
> 证明：
>
> > $$
> > F=\mathbb{E}_{x\sim p(x,\theta)}[\nabla_\theta \log p(x|\theta)\nabla_\theta \log p(x|\theta)^T]
> > $$
> > $$
> > \begin{aligned}
> > H_{\log p(x|\theta)}&=\nabla_\theta(\nabla_\theta \log p(x|\theta))\\
> > &=\nabla_\theta(\frac{\nabla_\theta p(x|\theta)}{p(x|\theta)})\\
> > &=\frac{p(x|\theta)\nabla_\theta^2 p(x|\theta-\nabla_\theta p(x|\theta)\nabla_\theta p(x|\theta)^T)}{p^2(x|\theta)}\\
> > &=\frac{\nabla_\theta^2 p(x|\theta)}{p(x|\theta)}-\nabla_\theta\log p(x|\theta)\nabla_\theta\log p(x|\theta)^T
> > \end{aligned}
> > $$
> > $$
> > \begin{aligned}
> > \mathbb{E}_{x\sim p(x,\theta)}[H_{\log p(x|\theta)}]&=\mathbb{E}_{x\sim p(x,\theta)}[\frac{\nabla_\theta^2 p(x|\theta)}{p(x|\theta)}]-F\\
> > &=\int\frac{\nabla_\theta^2 p(x|\theta)}{p(x|\theta)}p(x|\theta)dx-F\\
> > &=\nabla_\theta^2\int p(x|\theta)dx-F\\
> > &=-F
> > \end{aligned}
> > $$
>
> **结论2**：Fisher矩阵F是KL散度的Hessian矩阵H
>
> 即对$D_{KL}$的二阶求导结果，即：
> $$
> \begin{aligned}
> KL[p_\theta||p_{\theta+d}] &\approx KL[p_\theta||p_{\theta+d}]+(\nabla_{\theta'}KL[p_\theta||p_{\theta'}]|_{\theta'=\theta})^Td+\frac{1}{2}d^TFd\\
> &=KL[p_\theta||p_{\theta+d}]-E_{p(x|\theta)}[\nabla_\theta\log p(x|\theta)]^Td+\frac{1}{2}d^TFd\\
> &=\frac{1}{2}d^TFd
> \end{aligned}
> $$
> 记$m(\theta)=E_{s \sim \rho_{\theta_{old}}}[D_{KL}(\pi_{\theta_{old}}(\cdot|s)||\pi_{\theta}(\cdot|s))] $，则$m(\theta)在\theta=\theta_{old}$处的二阶泰勒展开为：
> $$
> \begin{aligned}
> m(\theta)& \approx m(\theta_{old})+\nabla_{\theta}m(\theta)|_{\theta=\theta_{old}}(\theta-\theta_{old})
> +\frac{1}{2}(\theta-\theta_{old})^T\nabla_{\theta}^2m(\theta)|_{\theta=\theta_{old}}(\theta-\theta_{old}) \longrightarrow 由前面的推导\\
> &=-\frac{1}{2}(\theta-\theta_{old})^T E_{s \sim\rho_{\theta_{old}}}[H_{\log \pi_{\theta_{old}}}](\theta-\theta_{old})\\
> &=\frac{1}{2}(\theta-\theta_{old})^T E_{s \sim\rho_{\theta_{old}}}[F_{\pi_{\theta_{old}}}](\theta-\theta_{old})
> \end{aligned}
> $$
> 另一种证明方法：
> $$
> \begin{aligned}
> KL[\log p(x|\theta)|\log p(x|\theta')]&=\int p(x|\theta)\log\frac{\log p(x\theta)}{\log p(x|\theta')}\\
> &=\mathbb{E}_{x\sim p(x,\theta)}[\log p(x|\theta)]-\mathbb{E}_{x\sim p(x,\theta)}[\log p(x|\theta')]
> \end{aligned}
> $$
>
> $$
> \nabla_{\theta'}KL[\log p(x|\theta)|\log p(x|\theta')]=-\nabla_{\theta'}\mathbb{E}_{x\sim p(x,\theta)}[\log p(x|\theta')]
> $$
>
> $$
> \begin{aligned}
> \nabla^2_{\theta'}KL[\log p(x|\theta)|\log p(x|\theta')]|_{\theta'=\theta}&=-\nabla^2_{\theta'}\mathbb{E}_{x\sim p(x,\theta)}[\log p(x|\theta')]|_{\theta'=\theta}\\
> &=-\mathbb{E}_{x\sim p(x,\theta)}{H_{\log p(x|\theta)}}\\
> &=F
> \end{aligned}
> $$

# 八、利用共轭梯度法求解最优更新量

对式（21）和（22）构造拉格朗日函数，即
$$
\mathcal{L}(\theta,\lambda)=-(\nabla_\theta L_{\theta_{old}}(\theta)|_{\theta=\theta_{old}} \cdot (\theta-\theta_{old}))+\lambda(\frac{1}{2}(\theta-\theta_{old})^TF(\theta_{old})(\theta-\theta_{old}) - \delta) \tag{23}
$$
利用KKT条件：
$$
\frac{\partial\mathcal{L}(\theta,\lambda)}{\partial \theta}=-\nabla_\theta L_{\theta_{old}}(\theta)|_{\theta=\theta_{old}}+\lambda F(\theta_{old})(\theta-\theta_{old})=0\\
\lambda \ge 0\\
\lambda (\frac{1}{2}(\theta-\theta_{old})^TF(\theta_{old})(\theta-\theta_{old})-\delta)=0\\
\frac{1}{2}(\theta-\theta_{old})^TF(\theta_{old})(\theta-\theta_{old})-\delta \le 0

\tag{24}
$$
令$d=\lambda(\theta-\theta_{old})$，可以看出$d与\theta-\theta_{old}$同向，则d为最优更新量的搜索方向，即满足：
$$
F(\theta_{old})d=\nabla_{\theta}L_{\theta_{old}}(\theta)|_{\theta=\theta_{old}}\\
或d=F^{-1}(\theta_{old})\nabla_{\theta}L_{\theta_{old}}(\theta)|_{\theta=\theta_{old}}  \tag{25}
$$
$$
\theta=\theta_{old}+\sqrt{\left.\frac{2\delta}{g^TF^{-1}g} \right.}F^{-1}g,其中g=-\nabla_\theta L_{\theta_{old}}(\theta) \tag{26}
$$



## 1)计算更新步长：

设步长为$\beta$，则：
$$
\delta=\frac{1}{2}(\beta d^*)^TF(\theta_{old})(\beta d^*) \Rightarrow\beta=\sqrt{\left.\frac{2\delta}{d^{*^T}Fd^*}  \right.}，\\
其中d^*=F^{-1}g,这里d^*=-d,g为式（26）中的g \tag{27}
$$

$$
\theta_{new}=\theta_{old}+\beta \cdot d^* \tag{28}
$$

## 2)计算搜索方向

式（25）是个线性方程组，如果直接求逆，算法复杂度很高，达到$O(n^3)$，其中n是矩阵大小，所以采用共轭梯度的方法来求解，即将求解线性方程组的问题转化为求解与之等价的二次函数极小值问题，具体如下：

> 首先构造目标函数：
> $$
> f(x)=\frac{1}{2}x^TAx+b^Tx,其中A=A^T为正定矩阵，其极小值点为Ax=b的解\\
> 其中b^T=-\nabla_{\theta}L_{\theta_{old}}(\theta)|_{\theta=\theta_{old}}=g \longrightarrow 式(26)这种的g，和具体算法过程中的g没有关系\\
> A=-E_{p(x|\theta_{old})}[\nabla_{\theta}\log p(x|\theta)\nabla \log p(x|\theta)^T]=-E_{p(x|\theta_{old})}[\nabla_\theta^2 D_{KL}(p(x|\theta_{old})||p(x|\theta))]=H_{KL_[{p(x|\theta_{old})||p(x|\theta])}}=F
> $$
> 具体算法过程：
>
> 第一步：给定初始迭代点$x^{(0)}$以及停止条件（阈值$\epsilon或最大迭代次数n$）
>
> 第二步：计算$g^{(0)}=\nabla f(x^{(0)})=Ax^{(0)}+b$，如果$g^{(0)}=0$则停止迭代，否则$d^{(0)}=-g^{(0)}$
>
> 第三步：for k=0 to n-1 do:
>
> ​	a)	 $\alpha_k=-\frac{(g^{(k)})^Td^{k}}{(d^k)^TAd^{k}}$
>
> ​	b)	$x^{(k+1)}=x^{(k)}+\alpha_kd^{(k)}$
>
> ​	c)	$g^{(k+1)}=\nabla f(x^{(k+1)})=Ax^{(k+1)}+b$，如果$g^{(k+1)}=0$，停止迭代
>
> ​	d)	$\beta_k=\frac{(g^{(k+1)})^TAd^{k}}{(d^k)^TAd^{k}}$
>
> ​	e)	$d^{(k+1)}=-g^{(k+1)}+\beta_kd^{(k)}$
>
> End for
>
> 输出$x^{n+1}$
>
> 
>
> 此外：
>
> a)、d)都需要计算$Ad^k$，需要计算和存储黑塞矩阵A，为了避免大矩阵出现，只计算$Ad^k$向量：
> $$
> H\mathcal{v}=\nabla_{\theta}\left(\left(\nabla_\theta(D_{KL}^{\mathcal{v}^{\pi_{\theta_{k}}}}(\pi_{\theta_{k}},\pi_{\theta'})) \right)^T \right)\mathcal{v}=\nabla_{\theta}\left(\left(\nabla_\theta(D_{KL}^{\mathcal{v}^{\pi_{\theta_{k}}}}(\pi_{\theta_{k}},\pi_{\theta'})) \right)^T \mathcal{v}\right)
> $$
> 即现用一阶梯度和向量$\mathcal{v}$点乘后再计算二阶梯度

# 九、线性搜索

由前所述，TRPO对目标函数进行了一阶近似，对约束条件进行了二阶近似，且将最大散度限制放宽到了平均散度限制，所以根据前一节介绍的算法得到的$\pi_{\theta_{new}}$的平均回报未必高于$\pi_{\theta_{old}}$的平均回报，或者KL散度可能没有达到限制条件。所以TRPO在每次迭代的最后进行一次线性搜索，以确保找到满足条件，即找到一个最小的非负整数i，使得：
$$
\theta_{k+1}=\theta_k+\alpha^i\sqrt{\left. \frac{2\delta}{x^TFx}\right.}x
$$
满足KL散度限制，且策略回报有提升，其中$\alpha \in (0,1)$是一个决定线性搜索长度的超参数，而i一般按顺序取1,2,3,……直到$\theta_{k+1}$满足条件。

# 十、TRPO算法流程

初始化策略网络参数$\theta$和价值网络参数$\omega$

for 序列 e=1 $\rightarrow$ E do:

​		用当前策略$\pi_{\theta_k}$采样轨迹$\left\{s_1,a_1,r_1,s_2,a_2,r_2,\cdots \right\}$

​		根据收集到的数据和价值网络估计每个状态动作对的优势函数$A(s_t,a_t)$

​		计算策略目标函数的梯度g

​		用共轭梯度法计算$x=-F^{-1}g$

​		用线性搜索找到一个i，并更新策略网络参数$\theta_{k+1}=\theta_k+\alpha^i\sqrt{\left. \frac{2\delta}{x^TFx}\right.}x,其中i \in \left\{1,2,\cdots ,K\right\}$为提升策略并满足KL距离限制的最小整数

​		更新价值网络参数（与Actor-Critic中的更新方法相同）

end for



# 参考资料

John Schulman  Trust Region Policy Optimization

张伟楠 沈键 俞勇 《动手学强化学习》 人民邮电出版社

邹伟 鬲玲 刘昱杓 《强化学习》 清华大学出版社

作者：Dreammaker 链接：https://zhuanlan.zhihu.com/p/605886935 来源：知乎

机智的王小鹏 链接：https://space.bilibili.com/169602174

