---
layout: post
title: "TRPO算法详解"
description: "详细解释置信域策略优化算法（Trust Region Policy Optimization,TRPO）"
author:     "Hao"
category: true
tagline: 
tags: [algorithm]
---

[toc]

# 一、算法背景

置信域策略优化算法（Trust Region Policy Optimization,TRPO）是一种基于策略的方法，即先对策略进行参数化，并设计衡量策略质量的指标或目标函数，然后通过梯度上升法来最大化这指标，让策略逼近局部最优。一般的策略梯度算法在沿着策略梯度更新参数时，可能因为步长太大，使策略变差。TRPO在更新参数的时候会先试探权重参数下一步要更新的位置是否失控，如果失控则调整步长，否则视该区域为置信域（Trust Region）,在该区域内能保障策略提升的单调性。

# 二、定义

## 策略评估(policy evaluation)

策略$\pi$下产生的一系列状态-动作对的预期累计回报：
$$
\eta(\pi)=E_{s_0,a_0,s_1,a_1,\cdots}[\sum_{t=0}^{\infty}\gamma^tr(s_t)]\\ \tag{1}
其中，
s_0为环境的初始状态，与策略无关，由环境自动生成，即s_0 \thicksim \rho(s_0)；\\

a_t\thicksim\pi(\cdot\mid s_t);

s_{t+1} \thicksim P(s_{t+1}\mid s_t,a_t);
$$


## 状态值函数(state value function)

$$
V_{\pi}(s_t)=E_{a_t,s_{t+1},\cdots}\lbrack \sum_{l=0}^{\infty}\gamma^lr(s_{t+l}) \rbrack \tag{2}
$$

## 状态-动作值函数(state-action value function)

$$
Q_{\pi}(s_t,a_t)=E_{s_{t+1},a_{t+1},\cdots}[\sum_{l=0}^{\infty}\gamma^lr(s_{t+l})] \tag{3}
$$

## 动作优势函数(advantage action function)

即状态s下使用动作a产生的回报与状态s时所有动作产生平均回报的差，衡量某个特定动作相对平均收益的优势
$$
A_{\pi}(s,a)=Q_{\pi}(s,a)-V_{\pi}(s) \tag{4}
$$

# 三、将新策略的回报表示为旧策略的回报+其他值

$$
& \eta(\widetilde{\pi})=\eta(\pi)+E_{s_0,a_0,\cdots \thicksim\widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^tA_{\pi}(s_t,a_t)]\\
& 其中,s_0 \thicksim \rho(s_0),a_t\thicksim\pi(\cdot\mid s_t),s_{t+1} \thicksim P(s_{t+1}\mid s_t,a_t) \tag{5}
$$

> 证明：
> $$
> \begin{aligned}
> & E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}A_{\pi}(s_t,a_t)] \\
> &=E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}[Q_{\pi}(s_t,a_t)-V_{\pi}(s_t)]]\\
> &=E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}(r(s_t)+\gamma V_\pi(s_{t+1})-V_\pi (s_t))]\\
> &=E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}r(s_t)]+E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}(\gamma V_\pi(s_{t+1})-V_\pi (s_t))]\\
> &=\eta(\widetilde{\pi})+E_{\tau \mid \widetilde{\pi}}[-V_{\widetilde{\pi}}(s_0)+\gamma V_{\widetilde{\pi}}(s_1)-\gamma V_{\widetilde{\pi}}(s_1)+\gamma^2 V_{\widetilde{\pi}}(s_2)+\cdots]\\
> &=\eta(\widetilde{\pi})+(-E_{s_0}[V_{\pi}(s_0)])\longrightarrow 此处s_0\thicksim \pi 等价于 s_0 \thicksim \widetilde{\pi}\\
> &=\eta(\widetilde{\pi})-\eta(\pi)\\
> 其中：\\
> & E_{s_0}[V_{\pi}(s_0)]\\
> &=E_{s_0}[E_{a_0,s_1,\cdots}[\sum_{t=0}^{\infty}\gamma^t r(s_{0+t})]]\\
> &=E_{a_0,s_1,\cdots}[\sum_{t=0}^{\infty}\gamma^t r(s_{0+t})]\\
> &=\eta(\pi)\\
> 证毕
> \end{aligned}
> $$
>

定义：
$$
\rho_{\pi}(s)=P(s_0=s)+\gamma P(s_1=s)+\gamma^2 P(s_2=s)+\cdots \tag{6}
$$
即每个状态的折扣访问频率（Discounted Visitation Frequencies），其将时间步上的累加，转为了状态上的累加。当$\gamma$为1时，可以将其理解为状态的占用度量。

将该定义带入式（5），得到：
$$
\eta(\widetilde{\pi})=\eta(\pi)+\sum_{s}\rho_{\widetilde{\pi}}(s)\sum_{a}\widetilde{\pi}(a|s)A_{\pi}(s,a) \tag{7}
$$

> 证明：
> $$
> \begin{aligned}
> \eta(\widetilde{\pi})&=\eta(\pi)+\sum_{t=0}^{\infty}\sum_{s}P(s_t=s|\widetilde{\pi})\sum_{a}[\widetilde{\pi}(a|s) \cdot \gamma ^t \cdot A_{\pi}(s,a)]\\
> &=\eta(\pi)+\sum_{s}\sum_{t=0}^{\infty}\gamma ^tP(s_t=s|\widetilde{\pi})\sum_{a}[\widetilde{\pi}(a|s) \cdot A_{\pi}(s,a)]\\
> &=\eta(\pi)+\sum_{s}\rho_{\widetilde{\pi}}(s) \cdot \sum_a\widetilde{\pi}(a|s) \cdot A_{\pi}(s,a)
> \end{aligned}
> $$

# 四、对$\eta(\widetilde{\pi})$近似，获得替代回报函数

如果先大量采样得到$\rho_{\widetilde{\pi}}(s)$，再验证式（7）右边第二项$\ge$0，计算量太大，需要对不同的$\widetilde{\pi}$都进行大量采样，而强化学习的目标就是减少采样次数。

考虑这样一种情况，将原回报函数中的$\rho_{\widetilde{\pi}}(s)$替换为$\rho_{\pi}(s)$，定义替换函数：
$$
L_{\pi}(\widetilde{\pi})=\eta(\pi)+\sum_{s}\rho_{\pi}(s)\cdot \sum_a\widetilde{\pi}(a|s)  A_{\pi}(s,a) \tag{8}
$$
当$\eta(\widetilde{\pi})$和$L_{\pi}(\widetilde{\pi})$相差很小时，两者可以相互替代，将其均看成$\widetilde{\pi}$的函数，$\widetilde{\pi}$和$\pi$均为$\theta$的函数，当$\eta(\widetilde{\pi})$和$L_{\pi}(\widetilde{\pi})$在$\pi_{\theta_{old}}$处一阶近似时，即：
$$
\begin{array}{l}
L_{\pi_{\theta \text { old }}}\left(\pi_{\theta_{\text {old }}}\right)=\eta\left(\pi_{\theta_{\text {old }}}\right) \tag{9} \\
\left.\nabla_{\theta} L_{\pi_{\theta_{\text {old }}}}\left(\pi_{\theta}\right)\right|_{\theta=\theta_{\text {old }}}=\left.\nabla_{\theta} \eta\left(\pi_{\theta}\right)\right|_{\theta=\theta_{\text {old }}}
\end{array}
$$

> 证明：
>
> 1）对于式(9)的第一个式子：
>
> ​			式（7）中的$\pi$和$\pi_{old}$是一样的，都是指的原来的策略，故：
> $$
> \eta(\pi_{old})=\eta(\pi_{old})+\sum_{s}\rho_{\pi_{old}}(s) \sum_a \pi_{old}(a|s) A_{\pi_{old}}(s,a)
> $$
> ​			其中,
> $$
> \sum_a \pi_{old}(a|s)A_{\pi_{old}}(s,a)=0
> $$
> ​			上式等号右边第二项为0，故式（9）第一个式子得证。
>
> 2）对于式（9）的第二个式子，分别让式（7）和（8）对$\theta$求偏导，得：

> $$
> \begin{aligned}
> \nabla_{\theta}\eta(\widetilde{\pi})|_{\theta=\theta_{\text {old }}}&=\nabla_{\theta} \eta(\pi_{\pi_{\theta_{old}}})+\sum_s \nabla_{\theta}\rho_{\widetilde{\pi}}(s)\sum_{a}\widetilde{\pi}(a|s)A_{\pi}(s,a)+\sum_{s}\rho_{\widetilde{\pi}}(s)\sum_{a}\nabla\widetilde{\pi}(a|s)A_{\pi}(s,a)  \\
> &(1)第一项\eta(\pi_{\theta_{old}})是常数，故\nabla\eta(\pi_{\theta_{old}})=0\\
> &(2)当\widetilde{\pi}=\pi_{old}时，即\widetilde{\pi}的参数\theta等于\pi_{old}的参数\theta_{old}时，\sum_{a}\widetilde{\pi}(a|s)A_{\pi}(s,a)=0，故有\\
> &=\sum_{s}\rho_{\widetilde{\pi}}(s)\sum_{a}\nabla\widetilde{\pi}(a|s)A_{\pi}(s,a)\\
> &代入\theta=\theta_{old}，即\widetilde{\pi}=\pi_{old}\\
> &=\sum_{s}\rho_{\pi_{\theta_{old}}}(s)\sum_{a}\nabla \widetilde{\pi}(a|s)A_{\pi_{\theta_{old}}}(s,a)\\
> \end{aligned}
> $$
>
> $$
> \nabla_{\theta}L_{\pi_{\theta_{old}}}(\widetilde{\pi})|_{\theta=\theta_{\text {old }}}=0+\sum_{s}\rho_{\pi_{\theta_{old}}}(s)\sum_{a}\nabla \widetilde{\pi}(a|s)A_{\pi_{\theta_{old}}}(s,a) \longrightarrow 原式第一项\eta(\pi_{\theta_{old}})是常数，故\nabla\eta(\pi_{\theta_{old}})=0
> $$
>
> 证毕。

当$\eta(\widetilde{\pi})$和$L_{\pi}(\widetilde{\pi})$在$\pi_{\theta_{old}}$处一阶近似时，则在$\pi_{old}$附近，改善L的策略也能改善原汇报函数$\eta$，只要步长控制在$\pi_{old}$合理的邻域内。

# 五、控制$\pi$和$\widetilde{\pi}$之间的散度小于$\alpha$，就能保证回报单调增长

如果要使用L回报函数替代$\eta$回报函数，则$\pi$和$\widetilde{\pi}$不能差太多，否则一阶近似邻域将非常小，导致极其小的步长，会使得训练变慢。

Kakade和Langford在2002年提出过一个保守策略迭代更新方案，可以为$\eta$更新提供明确的下界，即对于策略改进采用以下混合方式时：
$$
\pi_{new}(a|s)=(1-\alpha)\pi_{old}(a|s)+\alpha \pi^`(a|s) \tag{10}
$$
其中，$\pi^`=arg \min_{\pi^`}L_{\pi_{old}}(\pi^`)$，有
$$
\eta(\pi_{new})\ge L_{\pi_{old}}-\frac{4\epsilon \gamma}{(1-\gamma)^2}\alpha^2,\epsilon=\max_{s}|E_{a \thicksim \widetilde{\pi}(a|s) }[A_{\pi}(s,a)]| \tag{11}
$$

> 证明：
>
> 首先定义$\overline{A}(s)$：
> $$
> \overline{A}(s)=E_{a \thicksim \widetilde{\pi}(a|s) }[A_{\pi}(s,a)]
> $$
> $\overline{A}(s)$表示在状态s时采用策略$\widetilde{\pi}$相对于之前策略的改进。
>
> 用$\overline{A}(s)$改写式(7)和(8)，得到：
> $$
> \begin{array}{l}
> \eta(\tilde{\pi})=\eta(\pi)+E_{\tau \sim \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} \bar{A}\left(s_{t}\right)\right] \\
> L_{\pi}(\tilde{\pi})=\eta(\pi)+E_{\tau \sim \pi}\left[\sum_{t=0}^{\infty} \gamma^{t} \bar{A}\left(s_{t}\right)\right]
> \end{array}
> $$
> 由于策略按照 $\pi_{new}(a|s)=(1-\alpha)\pi_{old}(a|s)+\alpha \pi^`(a|s)的模式混合，假设新策略\widetilde{\pi}是由\pi_{old}和\pi^`$各自按照一定权重进行混合的，策略可以表示为策略对$(\pi,\widetilde{\pi})$，由策略对产生的动作对$(a,\widetilde{a})$。
>
> 从这种视角看，产生动作$\widetilde{a}$的概率为$\alpha$,因为不同策略也可能产生相同的动作，所以在改进的策略$\pi_{new}$中，产生和原策略的动作（即a）不同的概率最多为$\alpha$，即$P(a\neq \widetilde{a}|s)\le\alpha$。
>
> 于是有：
> $$
> \begin{aligned}
> \overline{A}&=E_{\widetilde{a} \sim \widetilde{\pi}}[A_{\pi}(s,\widetilde{a})] \longrightarrow 定义\\
> &=E_{(a,\widetilde{a}) \sim (\pi,\widetilde{\pi})}[A_{\pi}(s,\widetilde{a})-A_{\pi}(s,a)] \longrightarrow E_{a \sim \pi}A_{\pi}(s,a)=0，所以这个等号就是减去了0\\
> &=P(a \ne \widetilde{a}|s)E_{(a,\widetilde{a}) \sim (\pi,\widetilde{\pi})|a \ne \widetilde{a}}[A_{\pi}(s,\widetilde{a})-A_{\pi}(s,a)]\longrightarrow 当a=\widetilde{a}时,A_{\pi}(s,\widetilde{a})-A_{\pi}(s,a)=0\\
> \end{aligned}
> $$
> 于是有：
> $$
> |\overline{A}|\le \alpha \cdot 2 \cdot \max_{s,a}|A_{\pi}(s,a)| \tag{12}
> $$
> 用$n_t$表示在时刻t之前策略$\pi$和$\widetilde{\pi}$产生的不同动作的数量，有：
> $$
> E_{s_t\sim\widetilde{\pi}}\left[\overline{A}\left(s_i\right)\right]=P\left(n_t=0\right)E_{s_t\sim\widetilde{\pi}| n_t=0}\left[\overline{A}\left(s_t\right)\right]+P\left(n_t>0\right)E_{s_t\sim\widetilde{\pi}| n_t>0}\left[\overline{A}\left(s_t\right)\right] \tag{13}
> $$
> 和
> $$
> E_{s_t\sim \pi}\left[\overline{A}\left(s_i\right)\right]=P\left(n_t=0\right)E_{s_t\sim \pi | n_t=0}\left[\overline{A}\left(s_t\right)\right]+P\left(n_t>0\right)E_{s_t\sim \pi | n_t>0}\left[\overline{A}\left(s_t\right)\right] \tag{14}
> $$
> $n_t=0$时，策略$\pi$和$\widetilde{\pi}$动作相同，将到达相同的状态，则有：
> $$
> E_{s_t\sim\widetilde{\pi}| n_t=0}\left[\overline{A}\left(s_t\right)\right]=E_{s_t\sim \pi | n_t=0}\left[\overline{A}\left(s_t\right)\right]
> $$
> 则式（13）减去式（14）有：
> $$
> \begin{aligned}
> |E_{s_t\sim\widetilde{\pi}| n_t>0}\left[\overline{A}\left(s_t\right)\right]-E_{s_t\sim \pi | n_t>0}\left[\overline{A}\left(s_t\right)\right]| &\le  |E_{s_t\sim\widetilde{\pi}| n_t>0}\left[\overline{A}\left(s_t\right)\right]|+|E_{s_t\sim \pi | n_t>0}\left[\overline{A}\left(s_t\right)\right]| \longrightarrow 使用式(12)的结论\\
> &\le 4\alpha \max_{s,a}|A_{\pi}(s,a)|
> \end{aligned}
> $$
> 又$P(n_t =0) \ge(1-\alpha)^{t}$，故$P(n_t>0) \le 1-(1-\alpha)^t$
>
> 于是有：
> $$
> \begin{aligned}
> |E_{s_t\sim\widetilde{\pi}}\left[\overline{A}\left(s_i\right)\right]-E_{s_t\sim \pi}\left[\overline{A}\left(s_i\right)\right]|&=P(n_t>0)|E_{s_t\sim\widetilde{\pi}| n_t>0}\left[\overline{A}\left(s_t\right)\right]-E_{s_t\sim \pi | n_t>0}\left[\overline{A}\left(s_t\right)\right]|\\
> & \le(1-(1-\alpha)^t) \cdot 4\alpha \max_{s,a}|A_{\pi}(s,a)|
> \end{aligned}
> $$
> 从而有：
> $$
> \begin{aligned}
> |\eta(\widetilde{\pi})-L_{\pi}(\widetilde{\pi})|&=\sum_{t=0}^{\infty}\gamma^t|E_{\tau \sim \widetilde{\pi}}[\overline{A}(s_t)]-E_{\tau \sim \pi}[\overline{A}(s_t)]|\\
> &\le \sum_{t=0}^{\infty}\gamma^t \cdot 4\epsilon\alpha(1-(1-\alpha)^t) \longrightarrow \epsilon=\max_{s}|E_{a \thicksim \widetilde{\pi}(a|s) }[A_{\pi}(s,a)]|\\
> &=4\epsilon\alpha(\frac{1}{1-\gamma}-\frac{1}{1-\gamma(1-\alpha)})\\
> &=\frac{4\alpha^2\gamma\epsilon}{(1-\gamma)(1-\gamma(1-\alpha))}\\
> &\le\frac{4\alpha^2\gamma\epsilon}{(1-\gamma)^2} 
> \end{aligned}
> $$
> 证毕。

由式（11）可知，可以保证在一定的误差范围内可以用$L_{\pi}(\widetilde{\pi})$代替$\eta(\widetilde{\pi})$。

但混合策略，即式（12）在实际应用中用得很少，个人理解是超参数$\alpha$很难设定，而$\alpha$其实表征的是两个策略之间的距离，而策略其实就是概率分布，衡量两个概率分布的相似程度自然而然想到散度，于是使用总方差散度(the Total Variation divergence)。

对于离散的取值，我们有：
$$
D_{TV}(p||q)=\frac{1}{2}\sum_{i}|p_i-q_i|\\
D_{TV}^{max}(\pi,\widetilde{\pi})=\max_s(\pi(\cdot|s)||\widetilde{\pi}(\cdot|s)) \tag{15}
$$
又有：
$$
[D_{TV}(p||q)]^2 \le D_{KL}(p||q)\\
D_{KL}^{max}(\pi,\widetilde{\pi})=\max_{s} D_{KL}(\pi(\cdot|s)||\widetilde{\pi}(\cdot|s))
$$
从而有：
$$
\eta(\widetilde{\pi}) \ge L_{\pi}(\widetilde{\pi})-CD_{KL}^{max}(\pi,\widetilde{\pi})\\
where C=\frac{4\epsilon\gamma}{(1-\gamma)^2}
$$
可以用KL散度控制$\pi$和$\widetilde{\pi}$之间的距离小于$\alpha$时，就能够在误差确定的情况下使用$L_{\pi}(\widetilde{\pi})$替代$\eta(\widetilde{\pi})$，从而在优化$L_{\pi}(\widetilde{\pi})$时，$\eta(\widetilde{\pi})$也在优化。

在保证回报函数单调不减的情况下，求取更新策略的算法：

> 算法：保证预期回报$\eta$不减的近似策略迭代算法
>
> 输入：初始化策略$\pi_0$
>
> For i=0,1,2,3,$\cdots$ until 收敛 do:
>
> ​		计算优势函数$A_{\pi_i}(s,a)$
>
> ​		求解如下约束问题：
>
> ​				$\pi_{i+1}=arg max_{\pi}(L_{\pi_{i}}(\pi)-\frac{4\epsilon\gamma}{(1-\gamma)^2}D_{KL}^{max}(\pi_i,\pi))$,
>
> ​						其中$\epsilon=\max_{s}|E_{a \thicksim \widetilde{\pi}(a|s) }[A_{\pi}(s,a)]$
>
> ​								$L_{\pi_{i}}(\pi_i)=\eta(\pi_i)+\sum_{s}\rho_{\pi_i}(s)\sum_{a}\pi(a|s)A_{\pi_i}(s,a)$
>
> End for

证明以上算法的有效性：

> 设$M_i(\pi)=L_{\pi_i}(\pi)-CD_{KL}^{max}(\pi_i,\pi)$
>
> 则$M_i(\pi_i)=L_{\pi_i}(\pi_i)=\eta(\pi_i) \longrightarrow D_{KL}^{max}(\pi_i,\pi_i)=0$
>
> 取$\pi_{i+1}=arg max_{\pi}(L_{\pi_i}(\pi)-C \cdot D_{KL}^{max}(\pi_i,\pi))$
>
> 则$\eta(\pi_{i+1}) \ge M_{i+1}(\pi_{i+1})$
>
> 于是有 $\eta(\pi_{i+1})-\eta(\pi_i) \ge M_i(\pi_{i+1}-M_i(\pi_i))$
>
> 故改善$M_i$也会改善$\eta$
>
> 证毕。

# 六、采取重要性采样，对算法进一步近似

参数化的策略是通过改变参数来优化目标函数，即实现$\max_{\theta}(L_{\theta_{old}}(\theta)-C \cdot D_{KL}^{max}(\theta_{old},\theta))$，可以改写为：
$$
\max_\theta L_{\theta_{old}}(\theta)\\
\text{subject to} D_{KL}^{max}(\theta_{old},\theta) \le \delta \tag{16}
$$
但上式的约束太严格，要求状态空间的每一点都维持在KL散度在一定范围内，所以在实际应用中用平均散度来作为最大KL散度的近似，这样就可以使用采样的方法，即：
$$
\overline{D}_{KL}^{\rho}(\theta_1,\theta_2):=E_{s \sim \rho}[D_{KL}(\pi_{\theta_1(\cdot |s)}||\pi_{\theta_2(\cdot |s)})] \tag{17}
$$
则有：
$$
\max_\theta \sum_s \rho_{\theta_{old}}(s)\sum_a\pi_\theta(a|s)A_{\theta_{old}}(s,a)\\
\text{subject to } \overline{D}_{KL}^{\rho_{\theta_{old}}}(\theta_{old},\theta) \le \delta \tag{18}
$$
