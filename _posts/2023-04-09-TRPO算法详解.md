---
layout: post
title: "TRPO算法详解"
description: "详细解释置信域策略优化算法（Trust Region Policy Optimization,TRPO）"
author:     "Hao"
category: true
tagline: 
tags: [algorithm]
---



# 算法背景



# 定义

## 策略评估(policy evaluation)

策略$\pi$下产生的一系列状态-动作对的预期累计回报：
$$
\eta(\pi)=E_{s_0,a_0,s_1,a_1,\cdots}[\sum_{t=0}^{\infty}\gamma^tr(s_t)]
$$
其中，

$s_0$为环境的初始状态，与策略无关，由环境自动生成，即$s_0 \thicksim \rho(s_0)$；

$a_t\thicksim\pi(\cdot\mid s_t)$;

$s_{t+1} \thicksim P(s_{t+1}\mid s_t,a_t)$;

## 状态值函数(state value function)

$$
V_{\pi}(s_t)=E_{a_t,s_{t+1},\cdots}\lbrack \sum_{l=0}^{\infty}\gamma^lr(s_{t+l}) \rbrack
$$

## 状态-动作值函数(state-action value function)

$$
Q_{\pi}(s_t,a_t)=E_{s_{t+1},a_{t+1},\cdots}[\sum_{l=0}^{\infty}\gamma^lr(s_{t+l})]
$$

## 动作优势函数(advantage action function)

即状态s下使用动作a产生的回报与状态s时所有动作产生平均回报的差，衡量某个特定动作相对平均收益的优势
$$
A_{\pi}(s,a)=Q_{\pi}(s,a)-V_{\pi}(s)
$$

# 将新策略的回报标示为旧策略的回报+其他

$$
\eta(\widetilde{\pi})=\eta(\pi)+E_{s_0,a_0,\cdots \thicksim\widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^tA_{\pi}(s_t,a_t)]
$$

其中，$s_0 \thicksim \rho(s_0)$，$a_t\thicksim\pi(\cdot\mid s_t)$,$s_{t+1} \thicksim P(s_{t+1}\mid s_t,a_t)$;

> 证明：
> $$
> E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}A_{\pi}(s_t,a_t)]=E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}[Q_{\pi}(s_t,a_t)-V_{\pi}(s_t)]]\\
> =E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}(r(s_t)+\gamma V_\pi(s_{t+1})-V_\pi (s_t))]\\
> =E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}r(s_t)]+E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}(\gamma V_\pi(s_{t+1})-V_\pi (s_t))]\\
> =\eta(\widetilde{\pi})
> $$
> 

