---
layout: post
title: "TRPO算法详解"
description: "详细解释置信域策略优化算法（Trust Region Policy Optimization,TRPO）"
author:     "Hao"
category: true
tagline: 
tags: [algorithm]
---

[toc]

# 一、算法背景

置信域策略优化算法（Trust Region Policy Optimization,TRPO）是一种基于策略的方法，即先对策略进行参数化，并设计衡量策略质量的指标或目标函数，然后通过梯度上升法来最大化这指标，让策略逼近局部最优。一般的策略梯度算法在沿着策略梯度更新参数时，可能因为步长太大，使策略变差。TRPO在更新参数的时候会先试探权重参数下一步要更新的位置是否失控，如果失控则调整步长，否则视该区域为置信域（Trust Region）,在该区域内能保障策略提升的单调性。

# 二、定义

## 策略评估(policy evaluation)

策略$\pi$下产生的一系列状态-动作对的预期累计回报：
$$
\eta(\pi)=E_{s_0,a_0,s_1,a_1,\cdots}[\sum_{t=0}^{\infty}\gamma^tr(s_t)]
$$
其中，

$s_0$为环境的初始状态，与策略无关，由环境自动生成，即$s_0 \thicksim \rho(s_0)$；

$a_t\thicksim\pi(\cdot\mid s_t)$;

$s_{t+1} \thicksim P(s_{t+1}\mid s_t,a_t)$;

## 状态值函数(state value function)

$$
V_{\pi}(s_t)=E_{a_t,s_{t+1},\cdots}\lbrack \sum_{l=0}^{\infty}\gamma^lr(s_{t+l}) \rbrack
$$

## 状态-动作值函数(state-action value function)

$$
Q_{\pi}(s_t,a_t)=E_{s_{t+1},a_{t+1},\cdots}[\sum_{l=0}^{\infty}\gamma^lr(s_{t+l})]
$$

## 动作优势函数(advantage action function)

即状态s下使用动作a产生的回报与状态s时所有动作产生平均回报的差，衡量某个特定动作相对平均收益的优势
$$
A_{\pi}(s,a)=Q_{\pi}(s,a)-V_{\pi}(s)
$$

# 三、将新策略的回报表示为旧策略的回报+其他值

$$
& \eta(\widetilde{\pi})=\eta(\pi)+E_{s_0,a_0,\cdots \thicksim\widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^tA_{\pi}(s_t,a_t)]\\
& 其中,s_0 \thicksim \rho(s_0),a_t\thicksim\pi(\cdot\mid s_t),s_{t+1} \thicksim P(s_{t+1}\mid s_t,a_t)
$$

> 证明：
> $$
> \begin{aligned}
> & E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}A_{\pi}(s_t,a_t)] \\
> &=E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}[Q_{\pi}(s_t,a_t)-V_{\pi}(s_t)]]\\
> &=E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}(r(s_t)+\gamma V_\pi(s_{t+1})-V_\pi (s_t))]\\
> &=E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}r(s_t)]+E_{\tau \mid \widetilde{\pi}}[\sum_{t=0}^{\infty}\gamma^{t}(\gamma V_\pi(s_{t+1})-V_\pi (s_t))]\\
> &=\eta(\widetilde{\pi})+E_{\tau \mid \widetilde{\pi}}[-V_{\widetilde{\pi}}(s_0)+\gamma V_{\widetilde{\pi}}(s_1)-\gamma V_{\widetilde{\pi}}(s_1)+\gamma^2 V_{\widetilde{\pi}}(s_2)+\cdots]\\
> &=\eta(\widetilde{\pi})+(-E_{s_0}[V_{\pi}(s_0)])\longrightarrow 此处s_0\thicksim \pi 等价于 s_0 \thicksim \widetilde{\pi}\\
> &=\eta(\widetilde{\pi})-\eta(\pi)\\
> 其中：\\
> & E_{s_0}[V_{\pi}(s_0)]\\
> &=E_{s_0}[E_{a_0,s_1,\cdots}[\sum_{t=0}^{\infty}\gamma^t r(s_{0+t})]]\\
> &=E_{a_0,s_1,\cdots}[\sum_{t=0}^{\infty}\gamma^t r(s_{0+t})]\\
> &=\eta(\pi)\\
> 证毕
> \end{aligned}
> $$
>

定义：
$$
\rho_{\pi}(s)=P(s_0=s)+\gamma P(s_1=s)+\gamma^2 P(s_2=s)+\cdots
$$
即每个状态的折扣访问频率（Discounted Visitation Frequencies），其将时间步上的累加，转为了状态上的累加。当$\gamma$为1时，可以将其理解为状态的占用度量。

将该定义带入式（5），得到：
$$
\eta(\widetilde{\pi})=\eta(\pi)+\sum_{s}\rho_{\widetilde{\pi}}(s)\cdot\sum_{a}\widetilde{\pi}(a|s)\cdot A_{\pi}(s,a)
$$

> 证明：
> $$
> \begin{aligned}
> \eta(\widetilde{\pi})&=\eta(\pi)+\sum_{t=0}^{\infty}\sum_{s}P(s_t=s|\widetilde{\pi})\sum_{a}[\widetilde{\pi}(a|s) \cdot \gamma ^t \cdot A_{\pi}(s,a)]\\
> &=\eta(\pi)+\sum_{s}\sum_{t=0}^{\infty}\gamma ^tP(s_t=s|\widetilde{\pi})\sum_{a}[\widetilde{\pi}(a|s) \cdot A_{\pi}(s,a)]\\
> &=\eta(\pi)+\sum_{s}\rho_{\widetilde{\pi}}(s) \cdot \sum_a\widetilde{\pi}(a|s) \cdot A_{\pi}(s,a)
> \end{aligned}
> $$

# 四、对$\eta(\widetilde{\pi})$近似，获得替代回报函数

如果先大量采样得到$\rho_{\widetilde{\pi}}(s)$，再验证式（8）右边第二项$\ge$0，计算量太大，需要对不同的$\widetilde{\pi}$都进行大量采样，而强化学习的目标就是减少采样次数。

考虑这样一种情况，将原回报函数中的$\rho_{\widetilde{\pi}}(s)$替换为$\rho_{\pi}(s)$，定义替换函数：
$$
L_{\pi}(\widetilde{\pi})=\eta(\pi)+\sum_{s}\rho_{\pi}(s)\cdot \sum_a\widetilde{\pi}(a|s)  A_{\pi}(s,a)
$$
当$\eta(\widetilde{\pi})$和$L_{\pi}(\widetilde{\pi})$相差很小时，两者可以相互替代，将其均看成$\widetilde{\pi}$的函数，$\widetilde{\pi}$和$\pi$均为$\theta$的函数，当$\eta(\widetilde{\pi})$和$L_{\pi}(\widetilde{\pi})$在$\pi_{\theta_0}$处一阶近似时，即：
$$

$$
